#!/usr/bin/python3

# Copyright (C) 2024 Red Hat, Inc.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import argparse
import asyncio
import contextlib
import json
import logging
import os
import platform
import sys
import tempfile
import traceback
from pathlib import Path
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Collection,
    Coroutine,
    Iterable,
    Never,
    Sequence,
)

import aiohttp
import tomllib

from lib.aio.github import GitHub, GitHubStatus
from lib.aio.httpqueue import HttpQueue
from lib.aio.jsonutil import (
    JsonError,
    JsonObject,
    get_int,
    get_nested,
    get_str,
    get_str_map,
    get_str_or_none,
    get_strv,
    json_merge_patch,
    typechecked,
)
from lib.aio.s3 import S3Key
from lib.aio.s3streamer import (
    ChunkedUploader,
    Destination,
    Index,
    LocalDestination,
    LocalStatus,
    S3Destination,
    Status,
)
from lib.directories import xdg_config_home

logger = logging.getLogger(__name__)

BOTS_DIR = Path(__file__).parent


class Failure(Exception):
    pass


class Job:
    def __init__(self, obj: JsonObject) -> None:
        # to checkout and post status
        self.repo = get_str(obj, 'repo')
        self.sha = get_str(obj, 'sha')
        self.context = get_str(obj, 'context')
        self.target = get_str_or_none(obj, 'target', None)

        # reporting
        self.slug = get_str_or_none(obj, 'slug', None)
        self.title = get_str_or_none(obj, 'title', None)

        # to run
        self.container = get_str_or_none(obj, 'container', None)
        self.command = get_strv(obj, 'command', None)
        self.env = get_str_map(obj, 'env')
        self.timeout = get_int(obj, 'timeout', 120)


class JobContext:
    config: JsonObject = {}  # immutable

    def load_config(self, path: Path | str, name: str, *, missing_ok: bool = False) -> None:
        logger.debug('Loading %s configuration from %s', name, str(path))
        try:
            with open(path, 'rb') as file:
                self.config = json_merge_patch(self.config, tomllib.load(file))
        except FileNotFoundError:
            if missing_ok:
                logger.debug('No %s configuration found at %s', name, str(path))
            else:
                sys.exit(f'No {name} configuration found at {path}')

    def __init__(self, config_file: str | None, session: aiohttp.ClientSession) -> None:
        # The config is made out of the built-in config...
        self.load_config(BOTS_DIR / 'job-runner.toml', 'built-in')

        # ... plus exactly one of the following:
        if config_file:
            self.load_config(config_file, 'command-line')
        elif config_file := os.environ.get('JOB_RUNNER_CONFIG'):
            self.load_config(config_file, '$JOB_RUNNER_CONFIG-specified')
        else:
            self.load_config(xdg_config_home('cockpit-dev/job-runner.toml'), 'user', missing_ok=True)

        self._session = session

        try:
            with get_nested(self.config, 'container') as container:
                self.container_cmd = (get_str(container, 'command'), *get_strv(container, 'args'))
                self.container_run_args = get_strv(container, 'run-args')
                self.default_image = get_str(container, 'default-image')

            with get_nested(self.config, 'logs') as logs:
                self.logs_driver = get_str(logs, 'driver')
                match self.logs_driver:
                    case 's3':
                        with get_nested(logs, 's3') as s3:
                            self.s3_url = get_str(s3, 'url')
                            with get_nested(s3, 'key') as key:
                                self.s3_key = S3Key(get_str(key, 'access'), get_str(key, 'secret'))
                            self.s3_acl = get_str(s3, 'acl')

                    case 'local':
                        with get_nested(logs, 'local') as local:
                            self.local_dir = os.path.expanduser(get_str(local, 'dir'))
                            self.local_link = get_str(local, 'link')

                    case other:
                        sys.exit(f'Unknown log driver {other}')

            with get_nested(self.config, 'status') as status:
                self.status_driver = get_str(status, 'driver')
                if self.status_driver not in ['local', 'github']:
                    sys.exit(f'Unknown status driver {self.status_driver}')

            with get_nested(self.config, 'github') as github:
                self.clone_url = get_str(github, 'clone')
                # only strictly require the token if reporting statuses
                if self.status_driver == 'github':
                    token = get_str(github, 'token')
                else:
                    token = get_str(github, 'token', '')

                self.github = GitHub(self._session,
                                     url=get_str(github, 'api'),
                                     token=token,
                                     user_agent=get_str(github, 'user-agent'))

        except JsonError as exc:
            sys.exit(f'Configuration error: {exc}')

    @contextlib.asynccontextmanager
    async def provide_context(self, job: Job) -> AsyncIterator[tuple[Destination, Status]]:
        destination: Destination
        status: Status

        slug = job.slug or f'{job.repo}/{job.context}/{job.sha}'

        async with HttpQueue(self._session) as queue:
            match self.logs_driver:
                case 's3':
                    destination = S3Destination(queue, self.s3_url + slug + '/', key=self.s3_key, acl=self.s3_acl)
                    log_url = destination.location + 'log.html'
                case 'local':
                    destination = LocalDestination(os.path.join(self.local_dir, slug) + '/')
                    log_url = self.local_link + slug + '/log.html'

            match self.status_driver:
                case 'local':
                    status = LocalStatus(log_url)
                case 'github':
                    status = GitHubStatus(self.github, queue, job.repo, job.sha, job.context, log_url)

            yield destination, status


async def timeout_minutes(minutes: float) -> Never:
    await asyncio.sleep(60 * minutes)
    raise Failure(f'Timeout after {minutes} minutes')


async def poll_state(api: GitHub, repo: str, sha: str, target: str) -> Never:
    def open_pr_targets(pulls: object) -> Collection[str]:
        # Given a list of pull requests (as from the `commits/abcdef/pulls`
        # endpoint) return a list of branches targeted by open PRs.
        def produce() -> Iterable[str]:
            if not isinstance(pulls, list):
                raise ValueError
            for pull in pulls:
                if not isinstance(pull, dict):
                    raise ValueError
                base = pull.get('base')
                if not isinstance(base, dict):
                    raise ValueError
                ref = base.get('ref')
                if not isinstance(ref, str):
                    raise ValueError

                if pull.get('state') == 'open':
                    yield ref

        return frozenset(produce())

    while True:
        targets = await api.get(f'repos/{repo}/commits/{sha}/pulls', open_pr_targets)
        if targets is not None and target not in targets:
            raise Failure(f'Commit no longer targets `{target}` in any open PR on `{repo}`')
        await asyncio.sleep(60)


async def gather_and_cancel(aws: Collection[Coroutine[None, None, None]]) -> None:
    tasks = {asyncio.create_task(coro) for coro in aws}

    try:
        (done,), tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
        done.result()  # to raise the exception, if applicable
    finally:
        for task in tasks:
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task


@contextlib.asynccontextmanager
async def spawn(args: Sequence[str], **kwargs: Any) -> AsyncIterator[asyncio.subprocess.Process]:
    logger.debug('spawn(%r)', args)
    process = await asyncio.create_subprocess_exec(*args, **kwargs)
    pid = process.pid
    logger.debug('spawn: pid %r', pid)
    try:
        yield process
    finally:
        logger.debug('spawn: waiting for pid %r', pid)
        status = await process.wait()
        logger.debug('spawn: pid %r exited, %r', pid, status)


async def run(args: Sequence[str], **kwargs: Any) -> None:
    logger.debug('run(%r)', args)
    try:
        process = await asyncio.create_subprocess_exec(*args, **kwargs)
        pid = process.pid
    finally:
        logger.debug('run: waiting for pid %r', pid)
        status = await process.wait()
        logger.debug('run: pid %r exited, %r', pid, status)


async def run_container(
    job: Job, ctx: JobContext, log: Callable[[bytes], None], index: Index,
) -> None:
    with tempfile.TemporaryDirectory() as tmpdir_path:
        tmpdir = Path(tmpdir_path)
        cidfile = tmpdir / 'cidfile'
        attachments = tmpdir / 'attachments'

        args = [
            *ctx.container_cmd, 'run',
            *ctx.container_run_args,
            f'--cidfile={cidfile}',
            '--interactive',
            *((f'--env={k}={v}' for k, v in job.env.items()) if job.env else ()),
            '--env=TEST_ATTACHMENTS=/tmp/attachments',
            job.container or ctx.default_image,
            'python3', '-c',
            (BOTS_DIR / 'checkout-and-run').read_text(),  # lulz
            f'--revision={job.sha}',
        ]

        if job.target:
            args.append(f'--rebase={job.target}')

        args.append(ctx.clone_url + job.repo)

        if job.command:
            args.append('--')
            args.extend(job.command)

        async with spawn(args, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT) as container:
            try:
                assert container.stdout is not None

                # Log data until we hit EOF
                while data := await container.stdout.read(1 << 20):  # 1MB
                    log(data)

                # Upload all attachments
                # TODO: live updates
                # TODO: invent async tarfile for StreamReader
                with contextlib.suppress(FileNotFoundError):
                    cid = cidfile.read_text().strip()
                    await run([*ctx.container_cmd, 'cp', '--', f'{cid}:/tmp/attachments/.', f'{attachments}'])
                    for file in attachments.rglob('*'):
                        index.write(str(file.relative_to(attachments)), file.read_bytes())

                if returncode := await container.wait():
                    raise Failure(f'Container exited with code {returncode}')

            finally:
                await run([*ctx.container_cmd, 'rm', '--force', '--time=0', f'--cidfile={tmpdir}/cidfile'],
                          stdout=asyncio.subprocess.DEVNULL, stderr=asyncio.subprocess.DEVNULL)


async def run_job(job: Job, ctx: JobContext) -> None:
    async with ctx.provide_context(job) as (destination, status):
        index = Index(destination)
        log = ChunkedUploader(index, 'log')

        try:
            title = job.title or f'{job.context}@{job.repo}#{job.sha}'
            log.start(title + '\n\nJob(' + json.dumps(job.__dict__, indent=4) + ')\n')
            status.post('pending', f'Testing in progress [{platform.node()}]')

            tasks = {run_container(job, ctx, log.write, index)}

            if job.timeout:
                tasks.add(timeout_minutes(job.timeout))

            if ctx.github and job.target:
                tasks.add(poll_state(ctx.github, job.repo, job.sha, job.target))

            await gather_and_cancel(tasks)

        except Failure as exc:
            log.write(f'\n*** Failure: {exc}\n'.encode())
            status.post('failure', str(exc))

        except asyncio.CancelledError:
            status.post('failure', f'Cancelled [{platform.node()}]')
            log.write('*** Job cancelled\n'.encode())
            raise

        except BaseException as exc:
            # ie: bug in this program, but let's be helpful
            status.post('failure', f'Internal error [{platform.node()}]')
            message = '\n\n' + '\n'.join(traceback.format_exception(exc)) + '\n'
            log.write(message.encode())
            raise

        else:
            status.post('success', 'Job successful')
            log.write('\n\nJob ran successfully.  :)\n'.encode())

        finally:
            log.write(b'', final=True)
            index.sync()


class KeyValueAction(argparse.Action):
    def __init__(self, option_strings: str, dest: str, **kwargs) -> None:
        super().__init__(option_strings, dest, **kwargs, default={})

    def __call__(
        self,
        parser: argparse.ArgumentParser,
        namespace: argparse.Namespace,
        values: str | Sequence[str] | None,
        option_string: str | None = None
    ) -> None:
        assert isinstance(values, str)
        key, eq, value = values.partition('=')
        if not eq:
            raise ValueError(f'--env parameter `{value}` must contain `=`')
        getattr(namespace, self.dest)[key] = value


async def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument('--debug', action='store_true', help="Enable debugging output")
    parser.add_argument('--config-file', '-F', metavar='FILENAME',
                        help="Config file [default JOB_RUNNER_CONFIG or ~/.config/cockpit-dev/job-runner.toml]")
    subprograms = parser.add_subparsers(dest='cmd', required=True, title="Subcommands")

    run_parser = subprograms.add_parser("run", help="Run a single job provided on the command line")
    run_parser.add_argument('--repo', required=True, help="The repository (like `cockpit-project/cockpit`)")
    run_parser.add_argument('--sha', required=True, help="The revision sha, exactly 40 hex digits")
    run_parser.add_argument('--context', required=True, help="The status we're reporting against the sha")
    run_parser.add_argument('--target', help="The target branch")
    run_parser.add_argument('--slug', help="The URL slug (used for logging)")
    run_parser.add_argument('--title', help="The title for the log page")
    run_parser.add_argument('--container', help="The container image to use (like `quay.io/cockpit/tasks:latest`)")
    run_parser.add_argument('--env', help="Environment variables for the test run", action=KeyValueAction)
    run_parser.add_argument('--timeout', type=int, help="Timeout of the job, in minutes", default=120)
    run_parser.add_argument('command', nargs='*', help="Command to run [default: .cockpit-ci/run]")

    run_parser = subprograms.add_parser("json", help="Run a single given as a JSON blob")
    run_parser.add_argument('json', help="The job, in JSON format")

    args = parser.parse_args()

    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    if args.cmd == 'run':
        # if this throws, it's an error in the parser setup, above
        job = Job(vars(args))

    elif args.cmd == 'json':
        try:
            job = Job(typechecked(json.loads(args.json), dict))
        except (JsonError, json.JSONDecodeError) as exc:
            sys.exit(f'Poorly formed job: {exc}')

    async with aiohttp.ClientSession() as session:
        ctx = JobContext(args.config_file, session)
        await run_job(job, ctx)

if __name__ == '__main__':
    asyncio.run(main())
