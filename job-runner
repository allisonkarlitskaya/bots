#!/usr/bin/python3

# Copyright (C) 2024 Red Hat, Inc.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import argparse
import asyncio
import contextlib
import json
import logging
import platform
import tempfile
import traceback
from pathlib import Path
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Collection,
    Coroutine,
    Iterable,
    Mapping,
    NamedTuple,
    Never,
    Sequence,
)

import aiohttp
import tomllib

from lib.aio.github import GitHub, GitHubStatus
from lib.aio.httpqueue import HttpQueue
from lib.aio.s3 import S3Key
from lib.aio.s3streamer import (
    ChunkedUploader,
    Destination,
    Index,
    LocalDestination,
    LocalStatus,
    S3Destination,
    Status,
)
from lib.aio.util import JsonObject, JsonValue
from lib.directories import xdg_config_home

logger = logging.getLogger(__name__)


class Config:
    _config: JsonObject

    def get(self, path: str) -> JsonValue:
        rest, sep, key = path.rpartition('.')
        base = self.get_obj(rest) if sep else self._config
        return base[key]

    def get_obj(self, path: str) -> JsonObject:
        value = self.get(path)
        assert isinstance(value, dict)
        return value

    def get_str(self, path: str) -> str:
        value = self.get(path)
        assert isinstance(value, str)
        return value

    def get_strv(self, path: str) -> Sequence[str]:
        value = self.get(path)
        assert isinstance(value, list) and all(isinstance(item, str) for item in value)
        return tuple(value)

    def __init__(self, config_file: str | None) -> None:
        if config_file is None:
            config_file = xdg_config_home('cockpit-dev/job-runner.toml', envvar='JOB_RUNNER_CONFIG')

        with open(config_file, 'rb') as file:
            self._config = tomllib.load(file)


class Failure(Exception):
    pass


class Job(NamedTuple):
    # to checkout and post status
    repo: str
    sha: str
    context: str
    target: str | None = None

    # reporting
    slug: str | None = None
    title: str | None = None

    # to run
    container: str | None = None
    env: Mapping[str, str] = {}
    timeout: int = 120  # minutes


class JobContext:
    container_cmd: Sequence[str]
    container_run_args: Sequence[str]

    github: GitHub | None

    def __init__(self, config: Config, session: aiohttp.ClientSession) -> None:
        self._session = session

        self.clone_url = 'https://github.com/'

        self.container_cmd = (config.get_str('container.command'), *config.get_strv('container.args'))
        self.container_run_args = config.get_strv('container.run-args')
        self.default_image = config.get_str('container.default-image')

        self.github = GitHub(self._session,
                             url=config.get_str('status.github.api'),
                             token=config.get_str('status.github.token'),
                             user_agent=config.get_str('status.github.user-agent'))

        self.s3_url = config.get_str('logs.s3.url')
        self.s3_key = S3Key(config.get_str('logs.s3.key.access'), config.get_str('logs.s3.key.secret'))
        self.s3_acl = config.get_str('logs.s3.acl')

    @contextlib.asynccontextmanager
    async def provide_context(self, job: Job) -> AsyncIterator[tuple[Destination, Status]]:
        destination: Destination
        status: Status

        if self.github:
            async with HttpQueue(self._session) as queue:
                slug = job.slug or f'{job.repo}/{job.context}/{job.sha}'
                destination = S3Destination(queue, self.s3_url + slug + '/', key=self.s3_key, acl=self.s3_acl)
                status = GitHubStatus(self.github, queue, job.repo, job.sha, job.context, destination.location + 'log.html')
                yield destination, status

        else:
            destination = LocalDestination('x/' + (job.slug or f'{job.repo}-{job.sha}-{job.context}'))
            status = LocalStatus(destination.location)
            yield destination, status


async def timeout_minutes(minutes: float) -> Never:
    await asyncio.sleep(60 * minutes)
    raise Failure(f'Timeout after {minutes} minutes')


async def poll_state(api: GitHub, repo: str, sha: str, target: str) -> Never:
    def open_pr_targets(pulls: object) -> Collection[str]:
        # Given a list of pull requests (as from the `commits/abcdef/pulls`
        # endpoint) return a list of branches targeted by open PRs.
        def produce() -> Iterable[str]:
            if not isinstance(pulls, list):
                raise ValueError
            for pull in pulls:
                if not isinstance(pull, dict):
                    raise ValueError
                base = pull.get('base')
                if not isinstance(base, dict):
                    raise ValueError
                ref = base.get('ref')
                if not isinstance(ref, str):
                    raise ValueError

                if pull.get('state') == 'open':
                    yield ref

        return frozenset(produce())

    while True:
        targets = await api.get(f'repos/{repo}/commits/{sha}/pulls', open_pr_targets)
        if targets is not None and target not in targets:
            raise Failure(f'Commit no longer targets `{target}` in any open PR on `{repo}`')
        await asyncio.sleep(60)


async def gather_and_cancel(aws: Collection[Coroutine[None, None, None]]) -> None:
    tasks = {asyncio.create_task(coro) for coro in aws}

    try:
        (done,), tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
        done.result()  # to raise the exception, if applicable
    finally:
        for task in tasks:
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task


@contextlib.asynccontextmanager
async def spawn(args: Sequence[str], **kwargs: Any) -> AsyncIterator[asyncio.subprocess.Process]:
    logger.debug('spawn(%r)', args)
    process = await asyncio.create_subprocess_exec(*args, **kwargs)
    pid = process.pid
    logger.debug('spawn: pid %r', pid)
    try:
        yield process
    finally:
        logger.debug('spawn: waiting for pid %r', pid)
        status = await process.wait()
        logger.debug('spawn: pid %r exited, %r', pid, status)


async def run(args: Sequence[str], **kwargs: Any) -> None:
    logger.debug('run(%r)', args)
    try:
        process = await asyncio.create_subprocess_exec(*args, **kwargs)
        pid = process.pid
    finally:
        logger.debug('run: waiting for pid %r', pid)
        status = await process.wait()
        logger.debug('run: pid %r exited, %r', pid, status)


async def run_container(
    job: Job, ctx: JobContext, log: Callable[[bytes], None], index: Index,
) -> None:
    with tempfile.TemporaryDirectory() as tmpdir_path:
        tmpdir = Path(tmpdir_path)
        cidfile = tmpdir / 'cidfile'
        attachments = tmpdir / 'attachments'

        args = (
            *ctx.container_cmd, 'run',
            *ctx.container_run_args,
            f'--cidfile={cidfile}',
            '--interactive',
            *((f'--env={k}={v}' for k, v in job.env.items()) if job.env else ()),
            '--env=TEST_ATTACHMENTS=/tmp/attachments',
            job.container or ctx.default_image,
            'python3', '-c',
            (Path(__file__).parent / 'checkout-and-run').read_text(),  # lulz
            ctx.clone_url + job.repo,
            *((job.sha,) if job.sha else ()),
            *((job.target,) if job.target else ()),
        )

        async with spawn(args, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT) as container:
            try:
                assert container.stdout is not None

                # Log data until we hit EOF
                while data := await container.stdout.read(1 << 20):  # 1MB
                    log(data)

                # Upload all attachments
                # TODO: live updates
                # TODO: invent async tarfile for StreamReader
                with contextlib.suppress(FileNotFoundError):
                    cid = cidfile.read_text().strip()
                    await run([*ctx.container_cmd, 'cp', '--', f'{cid}:/tmp/attachments/.', f'{attachments}'])
                    for file in attachments.rglob('*'):
                        index.write(str(file.relative_to(attachments)), file.read_bytes())

                if returncode := await container.wait():
                    raise Failure(f'Container exited with code {returncode}')

            finally:
                await run([*ctx.container_cmd, 'rm', '--force', '--time=0', f'--cidfile={tmpdir}/cidfile'],
                          stdout=asyncio.subprocess.DEVNULL, stderr=asyncio.subprocess.DEVNULL)


async def run_job(job: Job, ctx: JobContext) -> None:
    async with ctx.provide_context(job) as (destination, status):
        index = Index(destination)
        log = ChunkedUploader(index, 'log')

        try:
            title = job.title or f'{job.context}@{job.repo}#{job.sha}'
            log.start(title + '\n\nJob(' + json.dumps(job._asdict(), indent=4) + ')\n')
            status.post('pending', f'Testing in progress [{platform.node()}]')

            tasks = {run_container(job, ctx, log.write, index)}

            if job.timeout:
                tasks.add(timeout_minutes(job.timeout))

            if ctx.github and job.target:
                tasks.add(poll_state(ctx.github, job.repo, job.sha, job.target))

            await gather_and_cancel(tasks)

        except Failure as exc:
            log.write(f'\n*** Failure: {exc}\n'.encode())
            status.post('failure', str(exc))

        except asyncio.CancelledError:
            status.post('failure', f'Cancelled [{platform.node()}]')
            log.write('*** Job cancelled\n'.encode())
            raise

        except BaseException as exc:
            # ie: bug in this program, but let's be helpful
            status.post('failure', f'Internal error [{platform.node()}]')
            message = '\n\n' + '\n'.join(traceback.format_exception(exc)) + '\n'
            log.write(message.encode())
            raise

        else:
            status.post('success', 'Job successful')
            log.write('\n\nJob ran successfully.  :)\n'.encode())

        finally:
            log.write(b'', final=True)
            index.sync()


class KeyValueAction(argparse.Action):
    def __init__(self, option_strings: str, dest: str, **kwargs) -> None:
        super().__init__(option_strings, dest, **kwargs, default={})

    def __call__(
        self,
        parser: argparse.ArgumentParser,
        namespace: argparse.Namespace,
        values: str | Sequence[str] | None,
        option_string: str | None = None
    ) -> None:
        assert isinstance(values, str)
        key, eq, value = values.partition('=')
        if not eq:
            raise ValueError(f'--env parameter `{value}` must contain `=`')
        getattr(namespace, self.dest)[key] = value


async def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument('--config-file', '-F', metavar='FILENAME',
                        help="Config file [default JOB_RUNNER_CONFIG or ~/.config/cockpit-dev/job-runner.toml]")
    subprograms = parser.add_subparsers(dest='cmd', required=True, title="Subcommands")

    run_parser = subprograms.add_parser("run", help="Run a single job provided on the command line")
    run_parser.add_argument('repo', help="The repository (like `cockpit-project/cockpit`)")
    run_parser.add_argument('sha', help="The revision sha, exactly 40 hex digits")
    run_parser.add_argument('context', help="The status we're reporting against the sha")
    run_parser.add_argument('--target', help="The target branch")
    run_parser.add_argument('--slug', help="The URL slug (used for logging)")
    run_parser.add_argument('--title', help="The title for the log page")
    run_parser.add_argument('--container', help="The container image to use (like `quay.io/cockpit/tasks:latest`)")
    run_parser.add_argument('--env', help="Environment variables for the test run", action=KeyValueAction)
    run_parser.add_argument('--timeout', help="Timeout of the job, in minutes")

    run_parser = subprograms.add_parser("json", help="Run a single given as a JSON blob")
    run_parser.add_argument('json', help="The job, in JSON format")

    args = parser.parse_args()

    config = Config(args.config_file)

    if args.cmd == 'run':
        job = Job(**{field: getattr(args, field) for field in Job._fields})

    elif args.cmd == 'json':
        job = Job(**json.loads(args.json))

    logging.basicConfig(level=logging.DEBUG)

    async with aiohttp.ClientSession() as session:
        ctx = JobContext(config, session)
        await run_job(job, ctx)

if __name__ == '__main__':
    asyncio.run(main())
